{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zyf0717/git_environment/STB_social_media_analytics\n"
     ]
    }
   ],
   "source": [
    "# Amend to set wd to STB_social_media_analytics.\n",
    "%cd /home/jia/Desktop/git/STB_social_media_analytics/experimentation/jiaxin_experiment/MFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from scrapy import Selector\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import utils\n",
    "from time import sleep\n",
    "import traceback\n",
    "from random import random\n",
    "from selenium import webdriver\n",
    "from datetime import datetime, date, timedelta\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_file_MFW.yml') as file:\n",
    "    configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "chromedriver_path = configs['General']['chromedriver_path']\n",
    "\n",
    "db_in_flag = configs['MFW']['db_in_flag']\n",
    "db_out_flag = configs['MFW']['db_out_flag']\n",
    "\n",
    "if db_in_flag == 'csv':\n",
    "    \n",
    "    ### FOR POC ONLY ###\n",
    "    poi_index = [1, 2, 3, 4]\n",
    "    poi_name = ['Gardens by the Bay', \n",
    "                'Orchard Road',\n",
    "                'Empress Place Building',\n",
    "                'Nagore Durgha Shrine'\n",
    "               ]\n",
    "    poi_url = ['https://www.mafengwo.cn/poi/5422487.html',\n",
    "               'https://www.mafengwo.cn/poi/11057.html',\n",
    "               'https://www.mafengwo.cn/poi/7645620.html',\n",
    "               'https://www.mafengwo.cn/poi/6034067.html'\n",
    "              ]\n",
    "    poi_df = pd.DataFrame({'poi_index':poi_index, \n",
    "                           'poi_name':poi_name, \n",
    "                           'poi_url':poi_url}\n",
    "                         )\n",
    "    ####################\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql']:\n",
    "    poi_df = pd.DataFrame()\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql'] or db_out_flag in ['sqlite', 'mysql']:\n",
    "    cnx = mysql.connector.connect(host=configs['General']['host'],\n",
    "                                  database=configs['General']['database'],\n",
    "                                  user=configs['General']['user'],\n",
    "                                  password=configs['General']['password']\n",
    "                                 )\n",
    "else:\n",
    "    cnx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlMFW:\n",
    "    attributes_col_names = ['POI_INDEX',\n",
    "                            'TOTAL_REVIEWS',\n",
    "                            'GOOD_REVIEWS',\n",
    "                            'AVG_REVIEWS',\n",
    "                            'BAD_REVIEWS',\n",
    "                            'ABOUT',\n",
    "                            'PRICE',\n",
    "                            'ADDRESS',\n",
    "                            'PHONE',\n",
    "                            'WEBSITE',\n",
    "                            'TRANSPORTATION_MODE',\n",
    "                            'TIME_RECOMMENDED',\n",
    "                            'OPENING_HOURS',\n",
    "                            'ATTRIBUTES_CRAWLED_TIME'\n",
    "                           ]\n",
    "\n",
    "    reviews_col_names = ['REVIEW_INDEX',\n",
    "                         'WEBSITE_INDEX',\n",
    "                         'POI_INDEX_MFW',\n",
    "                         'REVIEWER_URL',\n",
    "                         'REVIEW_RATING',\n",
    "                         'REVIEW_DATE',\n",
    "                         'REVIEW_TIME',\n",
    "                         'REVIEW_TITLE',\n",
    "                         'REVIEW_BODY',\n",
    "                         'TRANSLATED_REVIEW_BODY_GOOGLE',\n",
    "                         'TRANSLATED_REVIEW_BODY_WATSON',\n",
    "                        ]\n",
    "    \n",
    "    reviewers_col_names = ['REVIEWER_URL',\n",
    "                           'REVIEWER_NAME',\n",
    "                           'REVIEWER_LEVEL',\n",
    "                           'GENDER',\n",
    "                           'IMAGE_URL',\n",
    "                           'CHINESE_RAW_LOCATION',\n",
    "                           'CLEANED_ENGLISH_LOCATION',\n",
    "                           'NUM_ATTENTIONS',\n",
    "                           'NUM_FANS',\n",
    "                           'REVIWER_UPDATED_TIME'\n",
    "                          ]\n",
    "    \n",
    "    \n",
    "    def __init__(self, chromedriver_path, poi_df, cnx, db_out_flag):\n",
    "        self.driver = webdriver.Chrome(chromedriver_path)\n",
    "        self.poi_df = poi_df\n",
    "        if cnx:\n",
    "            self.cursor = cnx.cursor()\n",
    "        self.db_out_flag = db_out_flag\n",
    "        self.number_of_pages = None\n",
    "        self.current_page = None\n",
    "        self.current_poi_index = None\n",
    "        self.review_final_page = False\n",
    "        self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "        self.reviews_df = pd.DataFrame(columns=self.reviews_col_names)\n",
    "        # Create unique CSVs.\n",
    "        self.datetime_string = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "        os.makedirs('./output/{}/'.format(self.datetime_string))\n",
    "        os.makedirs('./output/{}/reviews'.format(self.datetime_string))\n",
    "        self.attributes_df.to_csv('./output/{}/attributes.csv'.format(self.datetime_string),mode='a', index=False)\n",
    "                \n",
    "    \n",
    "    def add_to_database(self):\n",
    "        # Read csv, add to database, then cnx.commit().\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def crawl_pois(self, number_of_pages=None):\n",
    "        if number_of_pages is not None:\n",
    "            self.number_of_pages = number_of_pages\n",
    "        for _, row in self.poi_df.iterrows():\n",
    "            print('########## {} ##########'.format(row['poi_name']))\n",
    "        # Create <POI_INDEX>.csv in reviews and reviewers folders.\n",
    "            self.current_poi_index = row['poi_index']\n",
    "            self.reviews_df.to_csv('./output/{}/reviews/{}.csv'.format(self.datetime_string, self.current_poi_index), mode='a', index=False)\n",
    "         \n",
    "        # Crawl\n",
    "            try:\n",
    "                self.driver.get(row['poi_url'])\n",
    "                self.crawl_attributes(row['poi_index'])\n",
    "                self.attributes_df.to_csv('./output/{}/attributes.csv'.format(self.datetime_string),mode='a', header=False, index=False)\n",
    "                self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "                self.crawl_reviews(row['poi_index'])\n",
    "                if self.db_out_flag != 'csv':\n",
    "                    self.add_to_database()\n",
    "            except Exception as e:\n",
    "                log_file = open('./output/{}/log.txt'.format(self.datetime_string), 'a+')\n",
    "                log_file.write('{}, {}, page: {}, {}\\n'.format(row['poi_index'], row['poi_name'], self.current_page, datetime.now()))\n",
    "                # log_file.write(repr(e) + '\\n')\n",
    "                log_file.write(traceback.format_exc() + '\\n')\n",
    "                log_file.close()\n",
    "                break\n",
    "    \n",
    "    def crawl_reviews(self, poi_index):\n",
    "        if self.number_of_pages is not None:\n",
    "            self.current_page = 1\n",
    "            for i in range(self.number_of_pages):\n",
    "                print('Page {}'.format(self.current_page))\n",
    "                self.crawl_reviews_1_page(poi_index)\n",
    "                self.reviews_to_csv()\n",
    "                self.current_page += 1\n",
    "                if self.review_final_page:\n",
    "                    self.review_final_page= False\n",
    "                    break\n",
    "        else:\n",
    "            self.current_page = 1\n",
    "            while not self.review_final_page:\n",
    "                print('Page {}'.format(self.current_page))\n",
    "                self.crawl_reviews_1_page(poi_index)\n",
    "                self.reviews_to_csv()\n",
    "                self.current_page += 1\n",
    "                if self.review_final_page:\n",
    "                    self.review_final_page= False\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    def crawl_attributes(self, poi_index):\n",
    "        driver = self.driver\n",
    "        \n",
    "        # Crawling attributes elements.\n",
    "        #Forming selector path\n",
    "    sleep(randint(1,5)) \n",
    "    sel = Selector(text=driver.page_source)\n",
    "    sleep(randint(1,5)) \n",
    "\n",
    "    res = sel.xpath('.//div[@class=\"container\"]')   \n",
    "\n",
    "    xpath_name='//div[@class=\"row row-top\"]//div[@class=\"title\"]/h1/text()'   #passed\n",
    "\n",
    "    xpath_eng='//div[@class=\"row row-top\"]//div[@class=\"en\"]/text()'    \n",
    "    eng = res.xpath(xpath_eng).extract_first() \n",
    "\n",
    "    xpath_total_reviews = '//div[@class=\"mhd mhd-large\"]/span/em/text()'    \n",
    "    total_reviews = res.xpath(xpath_total_reviews).extract_first() \n",
    "\n",
    "    xpath_good_reviews = '//li[@data-category=\"13\"]/a/span[@class=\"num\"]/text()'   \n",
    "    good_reviews = res.xpath(xpath_good_reviews).extract_first()\n",
    "\n",
    "    xpath_avg_reviews = '//li[@data-category=\"12\"]/a/span[@class=\"num\"]/text()'   \n",
    "    avg_reviews = res.xpath(xpath_avg_reviews).extract_first()\n",
    "\n",
    "    xpath_bad_reviews = '//li[@data-category=\"11\"]/a/span[@class=\"num\"]/text()'  \n",
    "    bad_reviews = res.xpath(xpath_bad_reviews).extract_first()\n",
    "    \n",
    "    #special case, there are many different special tags\n",
    "    #even worse, may share same xpath\n",
    "    xpath_special_reviews = '//li[@data-category=\"1\"]/a/span[@class=\"num\"]/text()'    \n",
    "    special_reviews = res.xpath(xpath_special_reviews).extract_first()\n",
    "\n",
    "    xpath_about1='//div[@data-cs-p=\"概况\"]/div/text()'  #specific location   \n",
    "    xpath_about2='//div[@class=\"main-detail\"]/dl[1]/dd/text()'   #general location\n",
    "    if res.xpath(xpath_about2).extract_first() != None:\n",
    "        about = (driver.find_element_by_xpath('//div[@class=\"main-detail\"]/dl[1]/dd')).text\n",
    "    else:\n",
    "        about = (driver.find_element_by_xpath('//div[@data-cs-p=\"概况\"]/div')).text\n",
    "\n",
    "    #xpath_price = the price shown here is a list and may not directly related to the POI(may be a travel package)\n",
    " \n",
    "\n",
    "    xpath_address1 ='//div[@class=\"mhd\"]/p/text()'   #景点位置   \n",
    "    xpath_address2 ='//div[@class=\"address\"]/text()'   #地址     \n",
    "    if res.xpath(xpath_address1).extract_first() != None:\n",
    "        address = res.xpath(xpath_address1).extract_first()\n",
    "    else:\n",
    "        address = res.xpath(xpath_address2).extract_first()\n",
    "\n",
    "\n",
    "    xpath_phone1 ='//li[@class=\"tel\"]/div[@class=\"content\"]/text()'   #specific location   \n",
    "    xpath_phone2 ='//li[@class=\"item-tel\"]/div[@class=\"content\"]/text()'   #general   \n",
    "    if res.xpath(xpath_phone1).extract_first() != None:   \n",
    "        phone = res.xpath(xpath_phone1).extract_first()\n",
    "    else:\n",
    "        phone = res.xpath(xpath_phone2).extract_first()                        \n",
    "\n",
    "\n",
    "    xpath_website = '//li[@class=\"item-site\"]/div[@class=\"content\"]/a/@href'  \n",
    "    website = res.xpath(xpath_website).extract_first()     \n",
    "\n",
    "\n",
    "    xpath_rec_time ='//li[@class=\"item-time\"]/div[@class=\"content\"]/text()'   #only appeared for specific location, but share sme xpath\n",
    "    if res.xpath(xpath_address1).extract_first() != None:    #confirm its a specific location\n",
    "        rec_time = res.xpath(xpath_rec_time).extract_first()\n",
    "    else:\n",
    "        rec_time = 'NA'\n",
    "    \n",
    "    xpath_op_hr1 = '//div[@class=\"mod mod-detail\"]/dl[3]/dd/text()'     #specific location case 1\n",
    "    xpath_op_hr1_1 = '//div[@class=\"mod mod-detail\"]/dl[3]/dd//div/text()'   #specific location case 2\n",
    "    xpath_op_hr2 ='//li[@class=\"item-time\"]/div[@class=\"content\"]/text()'\n",
    "    if res.xpath(xpath_address1).extract_first() != None:   #confirm it is a specific location\n",
    "        if res.xpath(xpath_op_hr1).extract_first() != None:\n",
    "            op_hr=res.xpath(xpath_op_hr1).extract_first()\n",
    "        else:\n",
    "            op_hr=res.xpath(xpath_op_hr1_1).extract_first()\n",
    "    else:\n",
    "        op_hr=res.xpath(xpath_op_hr2).extract_first()  #general\n",
    "    \n",
    "    \n",
    "    xpath_trans1 = '//div[@class=\"mod mod-detail\"]/dl[1]/dd/text()'    #specific location   \n",
    "    xpath_trans2 = '//div[@class=\"main-detail\"]/dl[3]/dd/text()'    #general   \n",
    "    if res.xpath(xpath_trans1).extract_first() != None:\n",
    "        transportation = res.xpath(xpath_trans1).extract_first()\n",
    "    else:\n",
    "        transportation = res.xpath(xpath_trans2).extract_first()\n",
    "\n",
    "        \n",
    "        # Parsing attributes.\n",
    "        \n",
    "        \n",
    "        poi_attributes = [poi_index,\n",
    "                          total_reviews, \n",
    "                          ranking, \n",
    "                          average_rating, \n",
    "                          rating_breakdown[0],\n",
    "                          rating_breakdown[1],\n",
    "                          rating_breakdown[2],\n",
    "                          rating_breakdown[3],\n",
    "                          rating_breakdown[4],\n",
    "                          about, \n",
    "                          address, \n",
    "                          datetime.now()\n",
    "                         ]\n",
    "        \n",
    "        # Inserting attributes into dataframe\n",
    "        poi_attributes_dict = dict(zip(self.attributes_col_names, poi_attributes))\n",
    "        self.attributes_df = self.attributes_df.append(poi_attributes_dict, ignore_index=True)\n",
    "\n",
    "    def crawl_reviews_1_page(self, poi_index, earliest_date=None):        \n",
    "        driver = self.driver\n",
    "        \n",
    "        # If crawl all languages, uncomment the follwing 3 lines.\n",
    "        # all_languages_button = driver.find_element_by_xpath('//span[@class=\"location-review-review-list-parts-LanguageFilter__no_wrap--2Dckv\"]')\n",
    "        # all_languages_button.click()\n",
    "        # sleep(1)\n",
    "        \n",
    "        read_more_button = driver.find_element_by_xpath('//span[@class=\"location-review-review-list-parts-ExpandableReview__cta--2mR2g\"]')\n",
    "        read_more_button.click()\n",
    "        sleep(1)\n",
    "\n",
    "        # Crawling review elements.\n",
    "        reviewer_url_elements = driver.find_elements_by_xpath('//a[@class=\"ui_header_link social-member-event-MemberEventOnObjectBlock__member--35-jC\"]')\n",
    "        reviewer_details_elements = driver.find_elements_by_xpath('//div[@class=\"social-member-event-MemberEventOnObjectBlock__event_wrap--1YkeG\"]')\n",
    "        review_id_elements = driver.find_elements_by_xpath('//div[@class=\"location-review-review-list-parts-SingleReview__mainCol--1hApa\"]')\n",
    "        review_rating_elements = driver.find_elements_by_xpath('//div[@class=\"location-review-review-list-parts-RatingLine__bubbles--GcJvM\"]/span')\n",
    "        review_title_elements = driver.find_elements_by_xpath('//a[@class=\"location-review-review-list-parts-ReviewTitle__reviewTitleText--2tFRT\"]')\n",
    "        review_body_elements = driver.find_elements_by_xpath('//div[@class=\"location-review-review-list-parts-ExpandableReview__containerStyles--1G0AE\"]')\n",
    "        date_of_experience_elements = driver.find_elements_by_xpath('//span[@class=\"location-review-review-list-parts-EventDate__event_date--1epHa\"]')\n",
    "        \n",
    "        for i in range(len(reviewer_url_elements)):\n",
    "            \n",
    "            # Parsing review and reviewer details\n",
    "            reviewer_url = reviewer_url_elements[i].get_attribute('href')\n",
    "            reviewer_name = reviewer_url_elements[i].text\n",
    "            review_id = self.parse_review_id_elements(review_id_elements[i].get_attribute('data-reviewid'))\n",
    "            review_date = self.parse_review_date(reviewer_details_elements[i].text)\n",
    "            location_contribution_votes = self.parse_location_contributions_votes(reviewer_details_elements[i].text)\n",
    "            review_rating = self.parse_review_rating(review_rating_elements[i].get_attribute('class'))\n",
    "            review_title = review_title_elements[i].text\n",
    "            review_body = self.parse_review_body(review_body_elements[i].text)\n",
    "            date_of_experience = self.parse_date_of_experience(review_body_elements[i].text)\n",
    "            trip_type = self.parse_trip_type(review_body_elements[i].text)\n",
    "            \n",
    "            review_details = [None, # REVIEW_INDEX\n",
    "                              1, # WEBSITE_INDEX (TripAdvisor is '1')\n",
    "                              poi_index,\n",
    "                              reviewer_url,\n",
    "                              review_id,\n",
    "                              review_date,\n",
    "                              review_rating,\n",
    "                              review_title,\n",
    "                              review_body,\n",
    "                              date_of_experience,\n",
    "                              trip_type,\n",
    "                              datetime.now()\n",
    "                             ]\n",
    "            \n",
    "            reviewer_details = [reviewer_url,\n",
    "                                reviewer_name,\n",
    "                                location_contribution_votes[0],\n",
    "                                None, # CLEANED_HOME_LOCATION\n",
    "                                location_contribution_votes[1],\n",
    "                                location_contribution_votes[2],\n",
    "                                datetime.now()\n",
    "                               ]\n",
    "            \n",
    "            # Inserting reviews into dataframe.\n",
    "            review_details_dict = dict(zip(self.reviews_col_names, review_details))\n",
    "            self.reviews_df = self.reviews_df.append(review_details_dict, ignore_index=True)\n",
    "            \n",
    "            # Inserting reviewers into dataframe.\n",
    "            reviewer_details_dict = dict(zip(self.reviewers_col_names, reviewer_details))\n",
    "            self.reviewers_df = self.reviewers_df.append(reviewer_details_dict, ignore_index=True)\n",
    "\n",
    "        next_button = driver.find_element_by_xpath('//a[@class=\"ui_button nav next primary \"]')\n",
    "        if next_button != []:\n",
    "            next_button.click()\n",
    "            sleep(1)\n",
    "    \n",
    "    # Methods below are all utility functions.\n",
    "    def calculate_total_reviews(self, rating_breakdown):\n",
    "        return sum(rating_breakdown)\n",
    "    \n",
    "    \n",
    "    def parse_ranking_text(self, text):\n",
    "        return int(text[1:text.find(' of')].replace(',', ''))\n",
    "    \n",
    "    \n",
    "    def calculate_average_rating(self, rating_breakdown):\n",
    "        total = sum(rating_breakdown)\n",
    "        average = 0\n",
    "        for i, j in enumerate(rating_breakdown[::-1]):\n",
    "            average += (i+1)*j/total\n",
    "        return average\n",
    "    \n",
    "    \n",
    "    def parse_rating_breakdown_elements(self, elements):\n",
    "        rating_breakdown = []\n",
    "        for element in elements:\n",
    "            text = element.text\n",
    "            rating_breakdown.append(int(text.replace(\",\", \"\")))\n",
    "        return rating_breakdown\n",
    "    \n",
    "    \n",
    "    def parse_address_text(self, text_1, text_2, text_3, text_4):\n",
    "        return ('{}, {}, {} {}'.format(text_1, text_2, text_3, text_4))\n",
    "    \n",
    "    \n",
    "    def parse_review_date(self, text):\n",
    "        date_string = text[text.find('wrote a review ')+15:text.find('\\n')]\n",
    "        \n",
    "        if date_string == 'Today':\n",
    "            return datetime.now().strftime('%d-%m-%Y')\n",
    "        elif date_string == 'Yesterday':\n",
    "            return (datetime.now() - timedelta(1)).strftime('%d-%m-%Y')\n",
    "        \n",
    "        re_search = re.search('(\\d+) (\\w+)', date_string)\n",
    "        current_year = datetime.now().strftime('%Y')\n",
    "        if re_search is not None:\n",
    "            if len(re_search.group(1)) == 1:\n",
    "                return datetime.strptime('0' + date_string + ' ' + current_year, '%d %b %Y').strftime('%d-%m-%Y')\n",
    "            else:\n",
    "                return datetime.strptime(date_string + ' ' + current_year, '%d %b %Y').strftime('%d-%m-%Y')\n",
    "        \n",
    "        return datetime.strptime(date_string, '%b %Y').strftime('%m-%Y')                     \n",
    "    \n",
    "    \n",
    "    def parse_location_contributions_votes(self, text):\n",
    "        location, contributions, votes = None, None, None\n",
    "        \n",
    "        votes_search = re.search('(\\d+) helpful votes?', text)\n",
    "        if votes_search is not None:\n",
    "            votes = int(votes_search.group(1))\n",
    "                \n",
    "        contributions_search = re.search('(\\d+) contributions?', text)\n",
    "        if contributions_search is not None:\n",
    "            contributions = int(contributions_search.group(1))\n",
    "\n",
    "        location_search = re.search('(.+?){} contributions?'.format(contributions), text)\n",
    "        if location_search is not None:\n",
    "            location = location_search.group(1)\n",
    "\n",
    "        return location, contributions, votes\n",
    "    \n",
    "    \n",
    "    def parse_review_id_elements(self, text):\n",
    "        return int(text)\n",
    "    \n",
    "    \n",
    "    def parse_review_rating(self, text):\n",
    "        return int(text[-2:])//10\n",
    "    \n",
    "    \n",
    "    def parse_review_body(self, text):\n",
    "        return text[:text.find('Read less')-1]\n",
    "    \n",
    "    \n",
    "    def parse_date_of_experience(self, text):\n",
    "        substring = re.search('Date of experience: (.+)\\n', text).group(1)\n",
    "        return datetime.strptime(substring, '%B %Y').strftime('%m-%Y')  \n",
    "    \n",
    "    \n",
    "    def parse_trip_type(self, text):\n",
    "        if text.find('Trip type: ') == -1:\n",
    "            return None\n",
    "        substring = text[text.find('Trip type: ')+11:]\n",
    "        return substring[:substring.find('\\n')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrawlTripAdvisor(chromedriver_path, poi_df, cnx, db_out_flag).crawl_pois(number_of_pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200116_152740'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', 'Jan']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'12 Jan'.split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
