{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jia/Desktop/git/STB_social_media_analytics/experimentation/jiaxin_experiment/QY\n"
     ]
    }
   ],
   "source": [
    "# Amend to set wd to STB_social_media_analytics.\n",
    "%cd /home/jia/Desktop/git/STB_social_media_analytics/experimentation/jiaxin_experiment/QY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4c8acf459d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from scrapy import Selector\n",
    "import re\n",
    "import yaml\n",
    "import utils\n",
    "from random import random\n",
    "from selenium import webdriver\n",
    "from datetime import datetime, date, timedelta\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_file_QY.yml') as file:\n",
    "    configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "chromedriver_path = configs['General']['chromedriver_path']\n",
    "\n",
    "db_in_flag = configs['QY']['db_in_flag']\n",
    "db_out_flag = configs['QY']['db_out_flag']\n",
    "\n",
    "if db_in_flag == 'csv':\n",
    "    \n",
    "    ### FOR POC ONLY ###\n",
    "    poi_index = [1, 2]\n",
    "    poi_name = ['Gardens by the Bay', \n",
    "                'Marina Bay Sands Hotel and Sands Skypark'\n",
    "               ]\n",
    "    poi_url = ['https://place.qyer.com/poi/V2UJY1FlBzZTZFI3/',\n",
    "               'https://place.qyer.com/poi/V2YJalFkBzBTbA/']\n",
    "    poi_df = pd.DataFrame({'poi_index':poi_index, \n",
    "                           'poi_name':poi_name, \n",
    "                           'poi_url':poi_url}\n",
    "                         )\n",
    "    ####################\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql']:\n",
    "    poi_df = pd.DataFrame()\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql'] or db_out_flag in ['sqlite', 'mysql']:\n",
    "    cnx = mysql.connector.connect(host=configs['General']['host'],\n",
    "                                  database=configs['General']['database'],\n",
    "                                  user=configs['General']['user'],\n",
    "                                  password=configs['General']['password']\n",
    "                                 )\n",
    "else:\n",
    "    cnx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlQY:\n",
    "    attributes_col_names = ['POI_INDEX',\n",
    "                            'TOTAL_REVIEWS',\n",
    "                            'ABOUT_SUMMARY',\n",
    "                            #'ABOUT',\n",
    "                            'RANKING',\n",
    "                            'TICKET_INFO',\n",
    "                            'RATES',\n",
    "                            'ADDRESS',\n",
    "                            'PHONE',\n",
    "                            'WEBSITE',\n",
    "                            'TRANSPORTATION_MODE',\n",
    "                            'OPENING_HOURS',\n",
    "                            'ATTRIBUTES_CRAWLED_TIME'\n",
    "                           ]\n",
    "\n",
    "    reviews_col_names = ['REVIEW_INDEX',\n",
    "                         'WEBSITE_INDEX',\n",
    "                         'POI_INDEX',\n",
    "                         'REVIEWER_URL',\n",
    "                         'REVIEW_RATING',\n",
    "                         'REVIEW_DATE'\n",
    "                         'REVIEW_BODY',\n",
    "                         #'TRANSLATED_REVIEW_BODY_GOOGLE',\n",
    "                         #'TRANSLATED_REVIEW_BODY_WATSON'\n",
    "                        ]\n",
    "    \n",
    "    #reviewers_col_names = ['REVIEWER_URL',\n",
    "                           #'REVIEWER_NAME',\n",
    "                           #'REVIEWER_LEVEL',\n",
    "                           #'GENDER',\n",
    "                           #'IMAGE_URL',\n",
    "                           #'CHINESE_RAW_LOCATION',\n",
    "                           #'CLEANED_ENGLISH_LOCATION',\n",
    "                           #'NUM_CITIES_VISITED',\n",
    "                           #'NUM_CTRIES_VISITED',\n",
    "                           #'REVIEWER_UPDATED_TIME'\n",
    "                          #]\n",
    "    \n",
    "    \n",
    "    def __init__(self, chromedriver_path, poi_df, cnx, db_out_flag):\n",
    "        self.driver = webdriver.Chrome(chromedriver_path)\n",
    "        if cnx is not None:\n",
    "            self.cursor = cnx.cursor()\n",
    "        self.poi_df = poi_df\n",
    "        self.db_out_flag = db_out_flag\n",
    "\n",
    "        self.number_of_pages = None\n",
    "        self.earliest_date = None\n",
    "        self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "        self.reviews_df = pd.DataFrame(columns=self.reviews_col_names)\n",
    "        #self.reviewers_df = pd.DataFrame(columns=self.reviewers_col_names)\n",
    "        \n",
    "        # Create unique CSVs.\n",
    "        self.datetime_string = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "        self.attributes_df.to_csv('./output/attributes_{}.csv'.format(self.datetime_string), mode='a', index=False)\n",
    "        self.reviews_df.to_csv('./output/reviews_{}.csv'.format(self.datetime_string), mode='a', index=False)\n",
    "        #self.reviewers_df.to_csv('./output/reviewers_{}.csv'.format(self.datetime_string), mode='a', index=False)\n",
    "                \n",
    "    \n",
    "    def add_to_database(self):\n",
    "        # Read csv, add to database, then cnx.commit().\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def crawl_pois(self, number_of_pages=None, earliest_date=None):\n",
    "        self.number_of_pages = number_of_pages\n",
    "        self.earliest_date = earliest_date\n",
    "        for _, row in poi_df.iterrows():\n",
    "            self.driver.get(row['poi_url'])\n",
    "            self.crawl_attributes(row['poi_index'])\n",
    "            self.crawl_reviews(row['poi_index'])\n",
    "            self.attributes_df.to_csv('./output/attributes_{}.csv'.format(self.datetime_string), mode='a', header=False, index=False)\n",
    "            self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "        \n",
    "    \n",
    "    def crawl_reviews(self, poi_index):\n",
    "        if self.earliest_date is not None:\n",
    "            # Crawl up till earliest_date.\n",
    "            pass\n",
    "        elif self.number_of_pages is not None:\n",
    "            for i in range(self.number_of_pages):\n",
    "                self.crawl_reviews_1_page(poi_index)\n",
    "                self.reviews_df.to_csv('./output/reviews_{}.csv'.format(self.datetime_string), mode='a', header=False, index=False)\n",
    "                #self.reviewers_df.to_csv('./output/reviewers_{}.csv'.format(self.datetime_string), mode='a', header=False, index=False)\n",
    "                self.reviews_df = pd.DataFrame(columns=self.reviews_col_names)\n",
    "                #self.reviewers_df = pd.DataFrame(columns=self.reviewers_col_names)\n",
    "        else:\n",
    "            # Crawl all pages.\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def crawl_attributes(self, poi_index):\n",
    "        driver = self.driver\n",
    "        \n",
    "        # Crawling attributes elements.\n",
    "        sleep(randint(1,3))\n",
    "        sel = Selector(text=driver.page_source)\n",
    "        sleep(randint(1,3))\n",
    "      \n",
    "        #select areas where we going to extract info\n",
    "        res = sel.xpath('.//div[@class=\"poi-detail\"]')    \n",
    "        \n",
    "        \n",
    "        #need parse ,\"xxx条点评\"\n",
    "        xpath_tot_reviews = '//a[@data-bn-ipg=\"place-poi-detail-viewAllReviews\"]/text()'     \n",
    "        total_review=res.xpath(xpath_tot_reviews).extract_first() \n",
    "\n",
    "        xpath_summary = '//div[@class=\"poi-detail\"]/div/p/text()'     \n",
    "        about_summary=res.xpath(xpath_summary).extract_first() \n",
    "        \n",
    "        #about is missing here, need to clarify the difference between about and about_summary\n",
    "        \n",
    "        \n",
    "        #need parse, \"第x名\",change to int\n",
    "        xpath_ranking = '//li[@class=\"rank\"]/span/text()'     \n",
    "        ranking_bf=res.xpath(xpath_ranking).extract_first() \n",
    "        \n",
    "        xpath_ticket = '//li/a/div/span[@class=\"price fontYaHei\"]/em/text()' \n",
    "        if res.xpath(xpath_ticket).extract_first() != None:\n",
    "            ticket_info=int(res.xpath(xpath_ticket).extract_first()) \n",
    "        else:\n",
    "            ticket_info = str(None)\n",
    "\n",
    "        #parse, remove spaces, change to float\n",
    "        xpath_rate = '//span[@class=\"number\"]/text()'    \n",
    "        rate=res.xpath(xpath_rate).extract_first()\n",
    "\n",
    "        xpath_add = '//li[1]/div[@class=\"content\"]/p/text()'   \n",
    "        address=res.xpath(xpath_add).extract_first()  \n",
    "\n",
    "        xpath_phone = '//li[5]/div[@class=\"content\"]/p/text()'    \n",
    "        phone=res.xpath(xpath_phone).extract_first() \n",
    "\n",
    "\n",
    "        xpath_web = '//li[6]/div[@class=\"content\"]/a/text()'     \n",
    "        website=res.xpath(xpath_web).extract_first() \n",
    "\n",
    "        xpath_trans = '//li[2]/div[@class=\"content\"]/p/text()'     \n",
    "        transportation_mode=res.xpath(xpath_trans).extract_first() \n",
    "  \n",
    "\n",
    "        xpath_openhr = '//li[3]/div[@class=\"content\"]/p/text()'   \n",
    "        opening_hours=res.xpath(xpath_openhr).extract_first()  \n",
    "        \n",
    "        # Parsing attributes.\n",
    "        total_reviews=self.parse_total_reviews(total_review)\n",
    "        ranking=self.parse_ranking(ranking_bf)\n",
    "        rates=self.parse_rate(rate)\n",
    "        \n",
    "        poi_attributes = [poi_index,\n",
    "                          total_reviews, \n",
    "                          about_summary, \n",
    "                          #about, \n",
    "                          ranking,\n",
    "                          ticket_info,\n",
    "                          rates,\n",
    "                          address,\n",
    "                          phone,\n",
    "                          website,\n",
    "                          transportation_mode,\n",
    "                          opening_hours,\n",
    "                          datetime.now()\n",
    "                         ]\n",
    "        \n",
    "        # Inserting attributes into dataframe\n",
    "        poi_attributes_dict = dict(zip(self.attributes_col_names, poi_attributes))\n",
    "        self.attributes_df = self.attributes_df.append(poi_attributes_dict, ignore_index=True)\n",
    "\n",
    "    def crawl_reviews_1_page(self, poi_index, earliest_date=None):        \n",
    "        driver = self.driver\n",
    "        \n",
    "      \n",
    "        # Crawling review elements.\n",
    "        sleep(randint(1,3))\n",
    "        sel = Selector(text=driver.page_source)\n",
    "        sleep(randint(1,3))\n",
    "        res = sel.xpath('//div[@class=\"compo-main compo-feed\"]') \n",
    "\n",
    "\n",
    "        xpath_url = './/a[@class=\"largeavatar\"]/@href'  \n",
    "        reviewer_urls = res.xpath(xpath_url).getall()   \n",
    "\n",
    "\n",
    "        xpath_rates = '//ul[@class=\"comment-list\"]//li//span[@class=\"poi-stars\"]'\n",
    "        star = res.xpath(xpath_rates).getall()\n",
    "        def regex_cnt(string,pattern):\n",
    "            return len(re.findall(pattern,string))\n",
    "        rates=list(map(lambda x:regex_cnt(x,\"singlestar full\"),star))\n",
    "       \n",
    "\n",
    "        xpath_date = '//a[@class=\"date\"]/text()'  \n",
    "        date = res.xpath(xpath_date).getall()   \n",
    "\n",
    "        xpath_body= '//p[@class=\"content\"]/text()'  \n",
    "       \n",
    "            \n",
    "        review_details = [None, # REVIEW_INDEX\n",
    "                          4, # WEBSITE_INDEX (QY is '4')\n",
    "                          poi_index,\n",
    "                          reviewer_url,\n",
    "                          review_rating,\n",
    "                          review_date,\n",
    "                          review_date,\n",
    "                          review_body,\n",
    "                          #translated_review_body_google,\n",
    "                          #translated_review_body_watson,\n",
    "                          datetime.now()\n",
    "                          ]\n",
    "            \n",
    "        #reviewer_details = [reviewer_url,\n",
    "                            #reviewer_name,\n",
    "                            #reviewer_level,\n",
    "                            #gender,\n",
    "                            #image_url,\n",
    "                            #chinese_raw_location,\n",
    "                            #cleaned-english_location,\n",
    "                            #num_cities_visited,\n",
    "                            #num_ctries_visited,\n",
    "                            #eviewer_update_time\n",
    "                            #datetime.now()\n",
    "                            #]\n",
    "            \n",
    "        # Inserting reviews into dataframe.\n",
    "        review_details_dict = dict(zip(self.reviews_col_names, review_details))\n",
    "        self.reviews_df = self.reviews_df.append(review_details_dict, ignore_index=True)\n",
    "            \n",
    "        # Inserting reviewers into dataframe.\n",
    "        #reviewer_details_dict = dict(zip(self.reviewers_col_names, reviewer_details))\n",
    "        #self.reviewers_df = self.reviewers_df.append(reviewer_details_dict, ignore_index=True)\n",
    "\n",
    "        next_page_button = driver.find_elements_by_xpath('//a[@title=\"下一页\"]')\n",
    "        if next_page_button:\n",
    "            sleep(randint(1,3))\n",
    "            next_page_button[0].click()  \n",
    "        else :\n",
    "            print(\"Can't click or last page\")\n",
    "    \n",
    "    # Methods below are all utility functions.\n",
    "    def parse_total_reviews(self, total_review_string):\n",
    "        return int(total_review_string[1:total_review_string.find('条点评')])\n",
    "    \n",
    "    def parse_ranking(self, ranking_string):\n",
    "        return int(ranking[2])\n",
    "    \n",
    "    def parse_rate(self, rate_string):\n",
    "        return float(rate_string.replace(' ', ''))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dec14a47e225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCrawlQY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromedriver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoi_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_out_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_pois\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_pages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-aa0a7b178f10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chromedriver_path, poi_df, cnx, db_out_flag)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchromedriver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoi_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_out_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromedriver_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcnx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "CrawlQY(chromedriver_path, poi_df, cnx, db_out_flag).crawl_pois(number_of_pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200116_152740'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', 'Jan']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'12 Jan'.split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
