{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jia/Desktop/git/STB_social_media_analytics/experimentation/jiaxin_experiment/QY\n"
     ]
    }
   ],
   "source": [
    "# Amend to set wd to STB_social_media_analytics.\n",
    "%cd /home/jia/Desktop/git/STB_social_media_analytics/experimentation/jiaxin_experiment/QY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from scrapy import Selector\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import utils\n",
    "from time import sleep\n",
    "import traceback\n",
    "from random import random\n",
    "from selenium import webdriver\n",
    "from datetime import datetime, date, timedelta\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_file_QY.yml') as file:\n",
    "    configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "chromedriver_path = configs['General']['chromedriver_path']\n",
    "\n",
    "db_in_flag = configs['QY']['db_in_flag']\n",
    "db_out_flag = configs['QY']['db_out_flag']\n",
    "\n",
    "if db_in_flag == 'csv':\n",
    "    \n",
    "    #df = pd.read_csv(\"trialQYlist.csv\")\n",
    "    \n",
    "    #def parse_reviews(text):\n",
    "     #   if re.search(r'\\d+',str(text)):\n",
    "      #      return int(re.search(r'\\d+', text).group())\n",
    "       # else:\n",
    "        #    return 0\n",
    "\n",
    "    #df['POI'] = df['POI'].apply(lambda x: x.replace(' ',''))    \n",
    "    #df['num_reviews'] = df['num_reviews'].apply(parse_reviews)\n",
    "    #df['link'] = df['link'].apply(lambda x: 'https://'+x[2:])\n",
    "    \n",
    "    #poi_index=list(range(len(df['POI'])))\n",
    "    \n",
    "    #poi_df = pd.DataFrame({'poi_index':poi_index, \n",
    "     #                      'poi_name':df['POI'], \n",
    "      #                     'poi_url':df['link']}\n",
    "       #                    )\n",
    "    \n",
    "    ### FOR POC ONLY ###\n",
    "    poi_index = [1, 2, 3, 4, 5]\n",
    "    poi_name = ['Gardens by the Bay',                        #2 popular POI, 2 not so popular ones, 1 404 test \n",
    "                'Marina Bay Sands Hotel and Sands Skypark',\n",
    "                'Thian Hock Keng Temple',\n",
    "                'Arab Street',\n",
    "                '404test'\n",
    "               ]\n",
    "    poi_url = ['https://place.qyer.com/poi/V2UJY1FlBzZTZFI3/',\n",
    "               'https://place.qyer.com/poi/V2YJalFkBzBTbA/',\n",
    "               'https://place.qyer.com/poi/V2EJZVFnBz9TYA/',\n",
    "               'https://place.qyer.com/poi/V2AJY1FmBzFTZg/',\n",
    "               'https://place.qyer.com/poi/V2AJY1jjtest/'\n",
    "              ]\n",
    "    poi_df = pd.DataFrame({'poi_index':poi_index, \n",
    "                           'poi_name':poi_name, \n",
    "                           'poi_url':poi_url}\n",
    "                           )\n",
    "    ####################\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql']:\n",
    "    poi_df = pd.DataFrame()\n",
    "\n",
    "if db_in_flag in ['sqlite', 'mysql'] or db_out_flag in ['sqlite', 'mysql']:\n",
    "    cnx = mysql.connector.connect(host=configs['General']['host'],\n",
    "                                  database=configs['General']['database'],\n",
    "                                  user=configs['General']['user'],\n",
    "                                  password=configs['General']['password']\n",
    "                                 )\n",
    "else:\n",
    "    cnx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlQY:\n",
    "    attributes_col_names = ['POI_INDEX',\n",
    "                            'TOTAL_REVIEWS',\n",
    "                            'ABOUT_SUMMARY',\n",
    "                            #'ABOUT',\n",
    "                            'RANKING',\n",
    "                            'TICKET_INFO',\n",
    "                            'RATES',\n",
    "                            'ADDRESS',\n",
    "                            'PHONE',\n",
    "                            'WEBSITE',\n",
    "                            'TRANSPORTATION_MODE',\n",
    "                            'OPENING_HOURS',\n",
    "                            'ATTRIBUTES_CRAWLED_TIME'\n",
    "                           ]\n",
    "\n",
    "    reviews_col_names = ['REVIEW_INDEX',\n",
    "                         'WEBSITE_INDEX',\n",
    "                         'POI_INDEX',\n",
    "                         'REVIEWER_URL',\n",
    "                         'REVIEW_RATING',\n",
    "                         'REVIEW_DATE',\n",
    "                         'REVIEW_BODY',\n",
    "                         'ATTRIBUTES_CRAWLED_TIME'\n",
    "                         #'TRANSLATED_REVIEW_BODY_GOOGLE',\n",
    "                         #'TRANSLATED_REVIEW_BODY_WATSON'\n",
    "                        ]\n",
    "    \n",
    "    #reviewers_col_names = ['REVIEWER_URL',\n",
    "                           #'REVIEWER_NAME',\n",
    "                           #'REVIEWER_LEVEL',\n",
    "                           #'GENDER',\n",
    "                           #'IMAGE_URL',\n",
    "                           #'CHINESE_RAW_LOCATION',\n",
    "                           #'CLEANED_ENGLISH_LOCATION',\n",
    "                           #'NUM_CITIES_VISITED',\n",
    "                           #'NUM_CTRIES_VISITED',\n",
    "                           #'REVIEWER_UPDATED_TIME'\n",
    "                          #]\n",
    "    \n",
    "    \n",
    "    def __init__(self, chromedriver_path, poi_df, cnx, db_out_flag):\n",
    "        self.driver = webdriver.Chrome(chromedriver_path)\n",
    "        self.poi_df = poi_df\n",
    "        if cnx:\n",
    "            self.cursor = cnx.cursor()\n",
    "        self.db_out_flag = db_out_flag\n",
    "        self.number_of_pages = None\n",
    "        self.current_page = None\n",
    "        self.current_poi_index = None\n",
    "        self.review_final_page = False\n",
    "        self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "        self.reviews_df = pd.DataFrame(columns=self.reviews_col_names)\n",
    "        self.sel=None\n",
    "         \n",
    "        \n",
    "        # Create unique CSVs.\n",
    "        self.datetime_string = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "        os.makedirs('./output/{}/'.format(self.datetime_string))\n",
    "        os.makedirs('./output/{}/reviews'.format(self.datetime_string))\n",
    "        self.attributes_df.to_csv('./output/{}/attributes.csv'.format(self.datetime_string),mode='a', index=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def add_to_database(self):\n",
    "        # Read csv, add to database, then cnx.commit().\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def crawl_pois(self, number_of_pages=None):\n",
    "        if number_of_pages is not None:\n",
    "            self.number_of_pages = number_of_pages\n",
    "        for _, row in self.poi_df.iterrows():\n",
    "            print('########## {} ##########'.format(row['poi_name']))\n",
    "        # Create <POI_INDEX>.csv in reviews and reviewers folders.\n",
    "            self.current_poi_index = row['poi_index']\n",
    "            self.reviews_df.to_csv('./output/{}/reviews/{}.csv'.format(self.datetime_string, self.current_poi_index), mode='a', index=False)\n",
    "         \n",
    "        # Crawl\n",
    "            try:\n",
    "                self.driver.get(row['poi_url'])\n",
    "    #################trial part to handle error of blank pages########################################\n",
    "                driver=self.driver\n",
    "                self.sel = Selector(text=driver.page_source)\n",
    "                sleep(1+random()*2)\n",
    "                res = self.sel.xpath('//div[@id=\"app\"]')\n",
    "                if  res == []:\n",
    "                    log_file = open('./output/{}/log.txt'.format(self.datetime_string), 'a+')\n",
    "                    log_file.write('{}, {}, {}, {}'.format(row['poi_index'],\n",
    "                                                           row['poi_name'],\n",
    "                                                           datetime.now(),\n",
    "                                                           'POI website not working'))\n",
    "                    continue\n",
    "        \n",
    "\n",
    "    ####################################################################################################\n",
    "                self.crawl_attributes(row['poi_index'])\n",
    "                self.attributes_df.to_csv('./output/{}/attributes.csv'.format(self.datetime_string),mode='a', header=False, index=False)\n",
    "                self.attributes_df = pd.DataFrame(columns=self.attributes_col_names)\n",
    "                self.crawl_reviews(row['poi_index'])\n",
    "                if self.db_out_flag != 'csv':\n",
    "                    self.add_to_database()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_file = open('./output/{}/log.txt'.format(self.datetime_string), 'a+')\n",
    "                log_file.write('{}, {}, page: {}, {}\\n'.format(row['poi_index'], row['poi_name'], self.current_page, datetime.now()))\n",
    "                # log_file.write(repr(e) + '\\n')\n",
    "                log_file.write(traceback.format_exc() + '\\n')\n",
    "                log_file.close()\n",
    "                break\n",
    "        \n",
    "    \n",
    "    def crawl_reviews(self, poi_index):\n",
    "        if self.number_of_pages is not None:\n",
    "            self.current_page = 1                               \n",
    "            for i in range(self.number_of_pages):\n",
    "                print('Page {}'.format(self.current_page))\n",
    "                self.crawl_reviews_1_page(poi_index)\n",
    "                self.reviews_to_csv()\n",
    "                self.current_page += 1\n",
    "                if self.review_final_page:\n",
    "                    self.review_final_page= False\n",
    "                    break\n",
    "        else:\n",
    "            self.current_page = 1\n",
    "            while not self.review_final_page:\n",
    "                print('Page {}'.format(self.current_page))\n",
    "                self.crawl_reviews_1_page(poi_index)\n",
    "                self.reviews_to_csv()\n",
    "                self.current_page += 1\n",
    "                if self.review_final_page:\n",
    "                    self.review_final_page= False\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "    def reviews_to_csv(self):\n",
    "        self.reviews_df.to_csv('./output/{}/reviews/{}.csv'.format(self.datetime_string, self.current_poi_index), mode='a', header=False, index=False)\n",
    "        self.reviews_df = pd.DataFrame(columns=self.reviews_col_names)\n",
    "    \n",
    "    def crawl_attributes(self, poi_index):\n",
    "      \n",
    "        #select areas where we going to extract info\n",
    "        res = self.sel.xpath('.//div[@class=\"poi-detail\"]')    \n",
    "        \n",
    "        \n",
    "        #need parse ,\"xxx条点评\"\n",
    "        xpath_tot_reviews = '//a[@data-bn-ipg=\"place-poi-detail-viewAllReviews\"]/text()'     \n",
    "        total_review=res.xpath(xpath_tot_reviews).extract_first() \n",
    "\n",
    "        xpath_summary = '//div[@class=\"poi-detail\"]/div/p/text()'\n",
    "        if res.xpath(xpath_summary).extract_first() != None:\n",
    "            about_summary=res.xpath(xpath_summary).extract_first() \n",
    "        else: \n",
    "            about_summary='Not Available'\n",
    "        \n",
    "        #about is missing here, need to clarify the difference between about and about_summary\n",
    "        \n",
    "        \n",
    "        #need parse, \"第x名\",change to int\n",
    "        xpath_ranking = '//li[@class=\"rank\"]/span/text()'     \n",
    "        ranking_bf=res.xpath(xpath_ranking).extract_first() \n",
    "        \n",
    "        xpath_ticket = './/li[./span[contains(text(),\"门票：\")]]/div/p/text()'\n",
    "        if res.xpath(xpath_ticket).extract_first() != None:\n",
    "            ticket_info=res.xpath(xpath_ticket).extract_first()\n",
    "        else:\n",
    "            ticket_info = str(None)\n",
    "\n",
    "        #parse, remove spaces, change to float\n",
    "        xpath_rate = '//span[@class=\"number\"]/text()'    \n",
    "        rate=res.xpath(xpath_rate).extract_first()\n",
    "        \n",
    "        xpath_address='.//li[./span[contains(text(),\"地址：\")]]/div/p/text()'\n",
    "        if res.xpath(xpath_address).extract_first() != None:\n",
    "            address=res.xpath(xpath_address).extract_first() \n",
    "        else:\n",
    "            address='Not Available'\n",
    "            \n",
    "        xpath_phone='.//li[./span[contains(text(),\"电话：\")]]/div/p/text()'\n",
    "        if res.xpath(xpath_phone).extract_first() != None:\n",
    "            phone=res.xpath(xpath_phone).extract_first() \n",
    "        else:\n",
    "            phone='Not Available'\n",
    "\n",
    "\n",
    "        xpath_web =  './/li[./span[contains(text(),\"网址：\")]]/div/a/@href'\n",
    "        if res.xpath(xpath_web).extract_first()!=None:\n",
    "            website=res.xpath(xpath_web).extract_first() \n",
    "        else:\n",
    "            website='Not Available'\n",
    "\n",
    "        #need edit, share same xpath with phone case 2\n",
    "        xpath_trans = './/li[./span[contains(text(),\"到达方式：\")]]/div/p/text()'  \n",
    "        if res.xpath(xpath_trans).extract_first() != None :\n",
    "            transportation_mode=res.xpath(xpath_trans).extract_first() \n",
    "        else:\n",
    "            transportation_mode='Not Available'\n",
    "  \n",
    "\n",
    "        xpath_openhr = './/li[./span[contains(text(),\"开放时间：\")]]/div/p/text()'\n",
    "        if res.xpath(xpath_openhr).extract_first() != None:\n",
    "            opening_hours=res.xpath(xpath_openhr).extract_first()  \n",
    "        else:\n",
    "            opening_hours='Not Available'\n",
    "        \n",
    "        # Parsing attributes.\n",
    "        total_reviews=self.parse_total_reviews(total_review)\n",
    "        ranking=self.parse_ranking(ranking_bf)\n",
    "        rates=self.parse_rate(rate)\n",
    "        \n",
    "        poi_attributes = [poi_index,\n",
    "                          total_reviews, \n",
    "                          about_summary, \n",
    "                          #about, \n",
    "                          ranking,\n",
    "                          ticket_info,\n",
    "                          rates,\n",
    "                          address,\n",
    "                          phone,\n",
    "                          website,\n",
    "                          transportation_mode,\n",
    "                          opening_hours,\n",
    "                          datetime.now()\n",
    "                         ]\n",
    "        \n",
    "        # Inserting attributes into dataframe\n",
    "        poi_attributes_dict = dict(zip(self.attributes_col_names, poi_attributes))\n",
    "        self.attributes_df = self.attributes_df.append(poi_attributes_dict, ignore_index=True)\n",
    "\n",
    "    def crawl_reviews_1_page(self, poi_index):        \n",
    "        driver=self.driver\n",
    "        self.sel = Selector(text=driver.page_source)\n",
    "        res1 = self.sel.xpath('.//div[@class=\"compo-main compo-feed\"]') \n",
    "        sleep(random()*2)\n",
    "        \n",
    "        xpath_url = './/a[@class=\"largeavatar\"]/@href' \n",
    "        reviewer_url = res1.xpath(xpath_url).getall()   \n",
    "        sleep(1+random()*2)\n",
    "\n",
    "        xpath_rates = './/ul[@class=\"comment-list\"]//li//span[@class=\"poi-stars\"]'\n",
    "        star = res1.xpath(xpath_rates).getall()\n",
    "        def regex_cnt(string,pattern):\n",
    "            return len(re.findall(pattern,string))\n",
    "        review_rating=list(map(lambda x:regex_cnt(x,\"singlestar full\"),star))\n",
    "       \n",
    "\n",
    "        xpath_date = './/a[@class=\"date\"]/text()'  \n",
    "        review_date1 = res1.xpath(xpath_date).getall()   \n",
    "\n",
    "        xpath_body= './/p[@class=\"content\"]/text()'  \n",
    "        review_body1= res1.xpath(xpath_body).getall()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Parsing reviews\n",
    "        review_body=list(map(self.parse_review_body,review_body1))\n",
    "        review_date=list(map(self.parse_review_date,review_date1))\n",
    "            \n",
    "        review_details = [None, # REVIEW_INDEX\n",
    "                          4, # WEBSITE_INDEX (QY is '4')\n",
    "                          poi_index,\n",
    "                          reviewer_url,\n",
    "                          review_rating,\n",
    "                          review_date,\n",
    "                          review_body,\n",
    "                          #translated_review_body_google,\n",
    "                          #translated_review_body_watson,\n",
    "                          datetime.now()\n",
    "                          ]\n",
    "            \n",
    "        #reviewer_details = [reviewer_url,\n",
    "                            #reviewer_name,\n",
    "                            #reviewer_level,\n",
    "                            #gender,\n",
    "                            #image_url,\n",
    "                            #chinese_raw_location,\n",
    "                            #cleaned-english_location,\n",
    "                            #num_cities_visited,\n",
    "                            #num_ctries_visited,\n",
    "                            #eviewer_update_time\n",
    "                            #datetime.now()\n",
    "                            #]\n",
    "            \n",
    "        # Inserting reviews into dataframe.\n",
    "        review_details_dict = dict(zip(self.reviews_col_names, review_details))\n",
    "        self.reviews_df = self.reviews_df.append(review_details_dict, ignore_index=True)\n",
    "            \n",
    "        # Inserting reviewers into dataframe.\n",
    "        #reviewer_details_dict = dict(zip(self.reviewers_col_names, reviewer_details))\n",
    "        #self.reviewers_df = self.reviewers_df.append(reviewer_details_dict, ignore_index=True)\n",
    "\n",
    "        next_page_button = self.driver.find_elements_by_xpath('//a[@title=\"下一页\"]')\n",
    "        if next_page_button:\n",
    "            sleep(1+random()*2)\n",
    "            self.driver.execute_script(\"arguments[0].click()\",next_page_button[0])\n",
    "            sleep(1+random()*2)\n",
    "            self.sel = Selector(text=self.driver.page_source)\n",
    "        else :\n",
    "            print(\"Can't click or last page\")\n",
    "            self.review_final_page= True\n",
    "         \n",
    "    \n",
    "    # Methods below are all utility functions.\n",
    "    def parse_total_reviews(self, text):\n",
    "        if re.search(r'\\d+',str(text)):\n",
    "            return int(re.search(r'\\d+', text).group())\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def parse_ranking(self, text):\n",
    "        return int(re.search(r'\\d+', text).group())\n",
    "    \n",
    "    def parse_rate(self, text):\n",
    "        if text:\n",
    "            return float(text.replace(' ', ''))\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def parse_review_date(self, text):\n",
    "        text1=text.replace('\\n                  ','')\n",
    "        return datetime.strptime(text1[0:10],\"%Y-%m-%d\")\n",
    "    \n",
    "    def parse_review_body(self, text):\n",
    "        return text.replace('\\n','')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Gardens by the Bay ##########\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "########## Marina Bay Sands Hotel and Sands Skypark ##########\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "########## Thian Hock Keng Temple ##########\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "########## Arab Street ##########\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "########## 404test ##########\n"
     ]
    }
   ],
   "source": [
    "CrawlQY(chromedriver_path, poi_df, cnx, db_out_flag).crawl_pois(number_of_pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
